{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2wM08wYt4tVZ",
        "TT29FMYa1Dcq",
        "Ip272cNB5L3C"
      ],
      "authorship_tag": "ABX9TyMg8tU3xpIxHnjsfm1O7twd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZackWongAPO/Colab/blob/main/LinearRegressionAir_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#加载库"
      ],
      "metadata": {
        "id": "2wM08wYt4tVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 库的加载\n",
        "\n",
        "# general\n",
        "import io\n",
        "\n",
        "# data processing\n",
        "import numpy\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# machine learning support\n",
        "import keras\n",
        "\n",
        "# data visualization\n",
        "import plotly.express as px\n",
        "import plotly.subplots as sb\n",
        "import plotly.graph_objects as go\n",
        "import seaborn\n",
        "\n",
        "from google.colab import files\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import LambdaCallback"
      ],
      "metadata": {
        "id": "5MJEoZml4v0I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#加载空气污染数据集"
      ],
      "metadata": {
        "id": "TT29FMYa1Dcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "XB4gacm21eRg",
        "outputId": "6d0ad4ef-6bcf-4267-c3d2-09be49cdb6de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a66fd48-e6b2-44d7-8747-26c8a496d9f3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a66fd48-e6b2-44d7-8747-26c8a496d9f3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving AirPollutionDefault.xlsx to AirPollutionDefault (1).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#数据预处理\n",
        ">数据预处理和特征工程详见pandas.ipynb，此处不再赘述，重点研究如何抽取训练集与测试集"
      ],
      "metadata": {
        "id": "ReXyR32z5EGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>由于数据与季节，年份有很大的关系，因此抽取数据分三种方式，这三个代码选择其一执行"
      ],
      "metadata": {
        "id": "WC5bH7tE7IJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 日期正常排列，不作任何处理\n",
        "df = pandas.read_excel('/content/AirPollutionDefault.xlsx')\n",
        "print(df.head())\n",
        "label = df['AQI']\n",
        "print(label.head())\n",
        "feature = df[['Date', 'PM25', 'PM10', 'SO2', 'CO', 'NO2', 'O3_8h']]\n",
        "print(feature.head())\n",
        "\n",
        "# X的意思是特征值，y的意思是标签值\n",
        "feature_train, feature_test, AQI_train, AQI_test = train_test_split(feature, label, test_size=0.15, random_state=123)\n",
        "features_train = feature_train.values\n",
        "features_test = feature_test.values\n",
        "AQIs_train = AQI_test.values\n",
        "AQIs_test = AQI_test.values\n",
        "print(type(feature_train))\n",
        "print(type(features_train))\n",
        "print(feature_train.head())\n",
        "print(feature_test.head())\n",
        "print(AQI_train.head())\n",
        "print(AQI_test.head())"
      ],
      "metadata": {
        "id": "bI-0alDb58tp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365fae07-4794-470d-8ae2-324f05d8f38f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Date  year  month  day  AQI   PM25  PM10   SO2   CO  NO2  O3_8h\n",
            "0     0  2013     12    2  145  111.0   163  89.0  1.7   85   42.0\n",
            "1     1  2013     12    3  356  306.0   299  98.0  1.9  127   21.0\n",
            "2     2  2013     12    4  314  264.0   230  98.0  1.5  144   24.0\n",
            "3     3  2013     12    5  170  129.0   162  59.0  1.3   80   36.0\n",
            "4     4  2013     12    6   55   39.0    61  45.0  0.9   76   36.0\n",
            "0    145\n",
            "1    356\n",
            "2    314\n",
            "3    170\n",
            "4     55\n",
            "Name: AQI, dtype: int64\n",
            "   Date   PM25  PM10   SO2   CO  NO2  O3_8h\n",
            "0     0  111.0   163  89.0  1.7   85   42.0\n",
            "1     1  306.0   299  98.0  1.9  127   21.0\n",
            "2     2  264.0   230  98.0  1.5  144   24.0\n",
            "3     3  129.0   162  59.0  1.3   80   36.0\n",
            "4     4   39.0    61  45.0  0.9   76   36.0\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'numpy.ndarray'>\n",
            "      Date  PM25  PM10   SO2   CO  NO2  O3_8h\n",
            "1318  1318  33.0    62   6.0  0.7   15  155.0\n",
            "1366  1366   8.0    35   8.0  0.3   19   84.0\n",
            "91      91  58.0    90  38.0  0.9   44   92.0\n",
            "2091  2091  12.0    38   7.0  0.4   28  101.0\n",
            "632    632  16.0    43  17.0  0.5   27   93.0\n",
            "      Date  PM25  PM10   SO2   CO  NO2  O3_8h\n",
            "243    243  24.0    41  12.0  0.5   24   70.0\n",
            "1267  1267  20.0    42   6.0  0.5   10   96.0\n",
            "452    452  14.0    32  15.0  0.6   19   72.0\n",
            "1324  1324  59.0   101   6.0  0.8   22  157.0\n",
            "1725  1725  14.0    38  10.0  0.5   18   84.0\n",
            "1318    95\n",
            "1366    42\n",
            "91      79\n",
            "2091    51\n",
            "632     47\n",
            "Name: AQI, dtype: int64\n",
            "243     41\n",
            "1267    48\n",
            "452     36\n",
            "1324    97\n",
            "1725    42\n",
            "Name: AQI, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 删除臭氧特征值，进行数据归一化后训练\n",
        "df = pandas.read_xls('/content/AirPollution01.xlsx')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "K6AEbwS37TUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 进一步拆分年月日，每周内按照随机数抽选训练集与测试集\n",
        "df = pandas.read_xls('/content/AirPollution01Final.xlsx')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "fjzGqWY87ZFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#编写画图函数"
      ],
      "metadata": {
        "id": "Ip272cNB5L3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义画图函数\n",
        "\n",
        "def make_plots(df, feature_names, label_name, model_output, sample_size=200):\n",
        "\n",
        "  random_sample = df.sample(n=sample_size).copy()\n",
        "  random_sample.reset_index()\n",
        "  weights, bias, epochs, rmse = model_output\n",
        "\n",
        "  is_2d_plot = len(feature_names) == 1\n",
        "  model_plot_type = \"scatter\" if is_2d_plot else \"surface\"\n",
        "  fig = make_subplots(rows=1, cols=2,\n",
        "                      subplot_titles=(\"Loss Curve\", \"Model Plot\"),\n",
        "                      specs=[[{\"type\": \"scatter\"}, {\"type\": model_plot_type}]])\n",
        "\n",
        "  plot_data(random_sample, feature_names, label_name, fig)\n",
        "  plot_model(random_sample, feature_names, weights, bias, fig)\n",
        "  plot_loss_curve(epochs, rmse, fig)\n",
        "\n",
        "  fig.show()\n",
        "  return\n",
        "\n",
        "def plot_loss_curve(epochs, rmse, fig):\n",
        "  curve = px.line(x=epochs, y=rmse)\n",
        "  curve.update_traces(line_color='#ff0000', line_width=3)\n",
        "\n",
        "  fig.append_trace(curve.data[0], row=1, col=1)\n",
        "  fig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\n",
        "  fig.update_yaxes(title_text=\"Root Mean Squared Error\", row=1, col=1, range=[rmse.min()*0.8, rmse.max()])\n",
        "\n",
        "  return\n",
        "\n",
        "def plot_data(df, features, label, fig):\n",
        "  if len(features) == 1:\n",
        "    scatter = px.scatter(df, x=features[0], y=label)\n",
        "  else:\n",
        "    scatter = px.scatter_3d(df, x=features[0], y=features[1], z=label)\n",
        "\n",
        "  fig.append_trace(scatter.data[0], row=1, col=2)\n",
        "  if len(features) == 1:\n",
        "    fig.update_xaxes(title_text=features[0], row=1, col=2)\n",
        "    fig.update_yaxes(title_text=label, row=1, col=2)\n",
        "  else:\n",
        "    fig.update_layout(scene1=dict(xaxis_title=features[0], yaxis_title=features[1], zaxis_title=label))\n",
        "\n",
        "  return\n",
        "\n",
        "def plot_model(df, features, weights, bias, fig):\n",
        "  df['FARE_PREDICTED'] = bias[0]\n",
        "\n",
        "  for index, feature in enumerate(features):\n",
        "    df['FARE_PREDICTED'] = df['FARE_PREDICTED'] + weights[index][0] * df[feature]\n",
        "\n",
        "  if len(features) == 1:\n",
        "    model = px.line(df, x=features[0], y='FARE_PREDICTED')\n",
        "    model.update_traces(line_color='#ff0000', line_width=3)\n",
        "  else:\n",
        "    z_name, y_name = \"FARE_PREDICTED\", features[1]\n",
        "    z = [df[z_name].min(), (df[z_name].max() - df[z_name].min()) / 2, df[z_name].max()]\n",
        "    y = [df[y_name].min(), (df[y_name].max() - df[y_name].min()) / 2, df[y_name].max()]\n",
        "    x = []\n",
        "    for i in range(len(y)):\n",
        "      x.append((z[i] - weights[1][0] * y[i] - bias[0]) / weights[0][0])\n",
        "\n",
        "    plane=pd.DataFrame({'x':x, 'y':y, 'z':[z] * 3})\n",
        "\n",
        "    light_yellow = [[0, '#89CFF0'], [1, '#FFDB58']]\n",
        "    model = go.Figure(data=go.Surface(x=plane['x'], y=plane['y'], z=plane['z'],\n",
        "                                      colorscale=light_yellow))\n",
        "\n",
        "  fig.add_trace(model.data[0], row=1, col=2)\n",
        "\n",
        "  return\n",
        "\n",
        "def model_info(feature_names, label_name, model_output):\n",
        "  weights = model_output[0]\n",
        "  bias = model_output[1]\n",
        "\n",
        "  nl = \"\\n\"\n",
        "  header = \"-\" * 80\n",
        "  banner = header + nl + \"|\" + \"MODEL INFO\".center(78) + \"|\" + nl + header\n",
        "\n",
        "  info = \"\"\n",
        "  equation = label_name + \" = \"\n",
        "\n",
        "  for index, feature in enumerate(feature_names):\n",
        "    info = info + \"Weight for feature[{}]: {:.3f}\\n\".format(feature, weights[index][0])\n",
        "    equation = equation + \"{:.3f} * {} + \".format(weights[index][0], feature)\n",
        "\n",
        "  info = info + \"Bias: {:.3f}\\n\".format(bias[0])\n",
        "  equation = equation + \"{:.3f}\\n\".format(bias[0])\n",
        "\n",
        "  return banner + nl + info + nl + equation\n",
        "\n",
        "print(\"SUCCESS: defining plotting functions complete.\")"
      ],
      "metadata": {
        "id": "vqAL4xVV1t9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#编写训练函数\n",
        ">包括机器学习模型的选择，深度学习神经元个数与层数的设计，模型的编译，训练超参数的输入等等"
      ],
      "metadata": {
        "id": "t_RS9ttt5VVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 封装各机器学习细节至总函数\n",
        "\n",
        "# 建立简单线性模型\n",
        "def build_linear_model(my_learning_rate, num_features):\n",
        "  # 大多数模型都是序列模型，无脑建模就可以\n",
        "  model = keras.models.Sequential()\n",
        "\n",
        "  # 添加全连接层\n",
        "  # unit表示神经元个数，只添加一层就只有一个输出层，一个神经元意味着只有一条直线\n",
        "  # input_shape 是输入数据的形状，对于大多数神经网络的输入层而言都是一维向量\n",
        "  model.add(keras.layers.Dense(units=1,input_shape=(num_features,)))\n",
        "\n",
        "  # 编译模型，从而加速训练速度\n",
        "\n",
        "  # optimizer：优化器，用于控制梯度裁剪。必选项。在此处修改流程图中的三种优化器方法\n",
        "  # 方法一：keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)（随机梯度下降法）\n",
        "  # 方法二：keras.optimizers.SGD(lr=0.01, momentum=0.5, decay=0.0, nesterov=True)（带动量版本的SGD）\n",
        "  # 方法三：keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)(理论最优)\n",
        "  # lr: float >= 0. 学习率。\n",
        "  # momentum : float>= 0. 动量值参数，用于加速 SGD 在相关方向上前进，并抑制震荡。\n",
        "  # decay: float >= 0. 每次参数更新后学习率衰减值，可有效防止模型不收敛。\n",
        "  # nesterov: boolean. 是否使用 Nesterov 动量。\n",
        "\n",
        "  # loss：损失函数（或称目标函数、优化评分函数）。必选项\n",
        "  # metrics：评价函数用于评估当前训练模型的性能。\n",
        "  # 当模型编译后（compile），评价函数应该作为 metrics 的参数来输入。\n",
        "  # 评价函数和损失函数相似，只不过评价函数的结果不会用于训练过程中，只应用于测试集。\n",
        "  model.compile(optimizer=keras.optimizers.SGD(learning_rate=my_learning_rate),\n",
        "                loss=keras.losses.MeanSquaredError,  # 选择差值平方作为Loss函数\n",
        "                metrics=[keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, features, label, epochs, batch_size):\n",
        "\n",
        "  # # 定义回调函数，当loss值不再明显变化的时候会提前结束训练\n",
        "  # early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "  # 开始训练模型\n",
        "  # batch_size：将一整个数据集随机分成多个小块(batch)，可以有效增加模型的收敛速度\n",
        "  # epochs：纪元数，相当于刷新权重值矩阵的次数\n",
        "  # x：输入特征值训练集，格式为Numpy数组\n",
        "  # y：输入标签值训练集，格式为Numpy数组\n",
        "  # 返回的是一个history对象，其属性值为连续epoch纪元值所对应的Loss值和评价值\n",
        "\n",
        "  history = model.fit( x=features,\n",
        "              y=label,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              # callbacks=[early_stopping]\n",
        "              )\n",
        "\n",
        "  # 得到最终训练结果的权重和偏差\n",
        "  trained_weight = model.get_weights()[0]\n",
        "  trained_bias = model.get_weights()[1]\n",
        "\n",
        "  # 返回纪元值列表\n",
        "  epochs = history.epoch\n",
        "  print(type(epochs))\n",
        "\n",
        "  # 将每个纪元值所对应的Loss值和评价值转化为pandas表格，用于作图\n",
        "  hist = pandas.DataFrame(history.history)\n",
        "\n",
        "  print(hist.head())\n",
        "  rmse = hist[\"root_mean_squared_error\"]\n",
        "\n",
        "  return trained_weight, trained_bias, epochs, rmse"
      ],
      "metadata": {
        "id": "Uz10wWRG5m0B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#编写验证函数\n",
        ">因为训练数据都是通过归一化的，所以预测时的输入应该也是归一化模型"
      ],
      "metadata": {
        "id": "KNGq2O8S5nIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "调用上述函数"
      ],
      "metadata": {
        "id": "XsNdQyVtwuee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = build_linear_model(0.01)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(model = linear_model,\n",
        "                                features = features_train,\n",
        "                                label = AQIs_train,\n",
        "                                epochs = 200,\n",
        "                                batch_size = 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "NlXqfTVNwx9e",
        "outputId": "88b424ca-5d21-4c2f-a830-e493ac831e41"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "build_linear_model() missing 1 required positional argument: 'num_features'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-216cb06501cd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinear_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_linear_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m trained_weight, trained_bias, epochs, rmse = train_model(model = linear_model, \n\u001b[1;32m      3\u001b[0m                                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAQI_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: build_linear_model() missing 1 required positional argument: 'num_features'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trained_weight)\n",
        "print(trained_bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wVxWzKKyozm",
        "outputId": "45240ed1-37fc-42ff-82d5-4b5b1ddbd992"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "[nan]\n"
          ]
        }
      ]
    }
  ]
}